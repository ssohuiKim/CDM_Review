services:
  frontend:
    build:
      context: ./frontend
    ports:
      - 15465:3000 # host:container
    restart: always   # ğŸ”¹ì„œë²„ ì¬ë¶€íŒ… or Docker ì¬ì‹œì‘ ì‹œ ìë™ ì‹¤í–‰

  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - 7800:7800 # host:container
    command: -m /models/gemma-2-2b-it-Q4_K_M.gguf --port 7800 --host 0.0.0.0 -n 512
    restart: always
    volumes:
      - ./localai/models:/models